{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchvision.io.read_image\n",
    "\n",
    "Reads a JPG image into a 3 dimensional RGB (channel, height, width). The values of the output tensor are uint8 in [0, 255].\n",
    "\n",
    "For example, the input is an image with width 2px, height 4px\n",
    "\n",
    "`print(input.shape)` output: (3, 4, 2)\n",
    "\n",
    "`print(input)` output\n",
    "```\n",
    "[[[248,   0],\n",
    "  [  0, 246],\n",
    "  [244,   0],\n",
    "  [240,   1]],\n",
    "\n",
    " [[  0, 248],\n",
    "  [  0, 244],\n",
    "  [  1, 242],\n",
    "  [239,   3]],\n",
    "\n",
    " [[  0,   1],\n",
    "  [246,   1],\n",
    "  [243, 243],\n",
    "  [237,   2]]\n",
    "]\n",
    "```\n",
    "\n",
    "`print(input)` expanded output\n",
    "```\n",
    "[\n",
    "  [\n",
    "    [248,   0],\n",
    "    [  0, 246],\n",
    "    [244,   0],\n",
    "    [240,   1]\n",
    "  ],\n",
    "\n",
    "  [\n",
    "    [  0, 248],\n",
    "    [  0, 244],\n",
    "    [  1, 242],\n",
    "    [239,   3]],\n",
    "\n",
    "  [\n",
    "    [  0,   1],\n",
    "    [246,   1],\n",
    "    [243, 243],\n",
    "    [237,   2]\n",
    "  ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.shape\n",
      "torch.Size([3, 4, 2])\n",
      "input\n",
      "tensor([[[248,   0],\n",
      "         [  0, 246],\n",
      "         [244,   0],\n",
      "         [240,   1]],\n",
      "\n",
      "        [[  0, 248],\n",
      "         [  0, 244],\n",
      "         [  1, 242],\n",
      "         [239,   3]],\n",
      "\n",
      "        [[  0,   1],\n",
      "         [246,   1],\n",
      "         [243, 243],\n",
      "         [237,   2]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image\n",
    "\n",
    "input = read_image(\"1-rgb2x4.jpg\")\n",
    "\n",
    "print(\"input.shape\")\n",
    "print(input.shape)\n",
    "print(\"input\")\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the difference between torchvision.io.read_image and cv2.imread?\n",
    "\n",
    "`torchvision.io.read_image` returns `(channel, height, width)`. `cv2.imread` returns `(height, width, channel)`\n",
    "\n",
    "`torchvision.io.read_image`\n",
    "1. `read_image_ouput[0]` returns `image_red` channel\n",
    "2. `read_image_output[1]` returns `image_green` channel\n",
    "3. `read_image_output[2]` returns `image_blue` channel\n",
    "\n",
    "`cv2.imread`\n",
    "1. `cv2.imread[:,:,0]` returns `image_blue` channel\n",
    "2. `cv2.imread[:,:,1]` returns `image_green` channel\n",
    "2. `cv2.imread[:,:,2]` returns `image_red` channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is torch.nn.Conv2d\n",
    "\n",
    "Applying a 2D convolution with `out-channels=4`, `kernel_size=(3,2)`, and `stride=1` over an input with width 4px, height 5px will output `(4, 3, 3)` where `(channel, height, width)`. `stride=1` will move the kernel by 1 pixel. So, an image with width 4px, height 5px, will generate new image with width 3px, height 3px\n",
    "\n",
    "`print(input[0])` output or Red channel\n",
    "```\n",
    "[[254, 255,   0, 255],\n",
    " [255,   0, 254, 251],\n",
    " [252, 254,   0, 255],\n",
    " [254,   1, 254, 255],\n",
    " [255, 255, 253, 255]]\n",
    "```\n",
    "\n",
    "`print(m.weight[0][0])` output or 1st kernel for Red channel.\n",
    "```\n",
    "[[ 0.1127, -0.0228],\n",
    " [-0.2330, -0.1881],\n",
    " [ 0.2010,  0.0741]]\n",
    "```\n",
    "\n",
    "In this case. the `torch.nn.Conv2d` have 4 filters. Each filter have 3 kernels. \n",
    "\n",
    "So, how filters `(4, 3, 3, 2)` become output `(4, 3, 3)`?\n",
    "\n",
    "Each new element is computed by elementwise multipying the highlighted input (blue) with the filter (red), summing it up, and then offsetting the result by the bias.\n",
    "\n",
    "## Convolution demo\n",
    "\n",
    "Reference:\n",
    "1. [Standford University CS231n](https://cs231n.github.io/convolutional-networks/)\n",
    "2. [University of Massachusetts Amherts](https://compsci682.github.io/notes/convolutional-networks/)\n",
    "\n",
    "![Convolution Demo](./3-convolution-demo.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input[0] with [0, 255] range\n",
      "tensor([[254, 255,   0, 255],\n",
      "        [255,   0, 254, 251],\n",
      "        [252, 254,   0, 255],\n",
      "        [254,   1, 254, 255],\n",
      "        [255, 255, 253, 255]], dtype=torch.uint8)\n",
      "m.weight.shape or kernel\n",
      "torch.Size([4, 3, 3, 2])\n",
      "m.weight[0]\n",
      "tensor([[-0.0153, -0.0690],\n",
      "        [-0.1429,  0.0850],\n",
      "        [-0.0061,  0.0010]], grad_fn=<SelectBackward0>)\n",
      "input.shape\n",
      "torch.Size([3, 5, 4])\n",
      "output.shape\n",
      "torch.Size([4, 3, 3])\n",
      "tensor([[[-0.4844, -0.0885, -0.3485],\n",
      "         [ 0.4091,  0.0825,  0.1730],\n",
      "         [-0.4535,  0.1701, -0.0270]],\n",
      "\n",
      "        [[ 0.3509,  0.6484,  0.4705],\n",
      "         [ 0.0720,  0.5867,  0.5814],\n",
      "         [ 0.2734,  0.3807,  0.4345]],\n",
      "\n",
      "        [[-0.4542, -0.8518, -0.6748],\n",
      "         [-0.6224, -0.2140, -0.8990],\n",
      "         [-0.6336, -0.2801, -0.5722]],\n",
      "\n",
      "        [[-0.1772, -0.1944,  0.1835],\n",
      "         [-0.4923, -0.1451,  0.0016],\n",
      "         [-0.0023, -0.3033, -0.0992]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# Define the Conv2d layer\n",
    "m = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=(3,2))\n",
    "\n",
    "# Reads a JPG image into a 3 dimensional RGB. The values of the output tensor are uint8 in [0, 255].\n",
    "input = read_image(\"2-rgb4x5.jpg\")\n",
    "print(\"input[0] with [0, 255] range\")\n",
    "print(input[0])\n",
    "# Normalize the values to [0, 1] range\n",
    "input = input.float() / 255.0\n",
    "\n",
    "# Perform the convolution\n",
    "output = m(input)\n",
    "\n",
    "# Print the input and output\n",
    "print(\"m.weight.shape or kernel\")\n",
    "print(m.weight.shape)\n",
    "print(\"m.weight[0]\")\n",
    "print(m.weight[0][0])\n",
    "print(\"input.shape\")\n",
    "print(input.shape)\n",
    "print(\"output.shape\")\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
